{
    "exp.framework": [
        "Framework to use. Supported frameworks: 'tensorflow', 'caffe2', 'mxnet', 'tensorrt',",
        "'nvidia_caffe', 'intel_caffe', 'bvlc_caffe'. May be overriden be experimenter if mutiple",
        "frameworks share the same backend implementation like all Caffe forks."
    ],
    "exp.model": "A model identifier to use such as 'alexnet', 'googlenet', 'vgg16' etc. Framework specific.",
    "exp.env": "Benchmarking environment - docker or bare metal. Possible values: 'docker' or 'host'",
    "exp.warmup_iters": "Number of warmup iterations to perform if supported by backend (For instance, Caffe does not support it).",
    "exp.bench_iters": "Number of benchmarking iterations.",
    "exp.phase": "Phase to benchmark. Possible values - 'inference' or 'training'",
    "exp.device_batch": "A device batch size.",
    "exp.gpus": "GPUs to use. List of comma-separated GPU identifiers. For instance '0' or '0,1' or '0,1,2,3' or '0,1,2,3,4,5,6,7'.",
    "exp.num_gpus": "Number of GPUs. Default value is computed based on ${exp.gpus} value.",
    "exp.device": "Device to use. Possible values - 'gpu' or 'cpu'. By default, is computed based on ${exp.num_gpus} value.",
    "exp.dtype": "Type of data to use if supported by a framework. Possible values 'float32', 'float16' or 'int8'.",
    "exp.enable_tensor_core": [
        "If true, enable tensor core operations for NVIDIA V100 and CUDA >= 9.0 if supported by a framework.",
        "Possible values 'true' or 'false'"
    ],
    "exp.simulation": "If true, do not run benchmark but print framework command line to a log file instead.",
    "exp.bench_root": "Root bechmark folder. Based on this relative path other paths, like log files, may be specified.",
    "exp.framework_id": "Unique framework identifier, default value is ${exp.framework}.",
    "exp.id": "UUID for a single benchamrk experiment.",
    "exp.effective_batch": "Effective batch size.",    
    "exp.exp_path": "Root folder where benchmark log files are stored.",
    "exp.log_file": "Benchmark log files.",
    "exp.force_rerun": "If log file exists and false, benchmark will not be ran.",
    "exp.docker.launcher": "One of 'nvidia-docker' or 'docker' depending on a ${exp.device} value.",

    "monitor.frequency": [
        "A sampling frequency in seconds of embedded resource monitor. By default (0) resource monitor is disabled. If this value is",
        "> 0, experimenter will start embedded resource monitor (kind of a reference implementation) that will log system parameters",
        "with this frequency. This parameters include cpu and memory consumption, power, GPU metrics etc.",
        "Assumption: in current implementation, if resource monitor is enabled for a first benchmark, it's considered to be enabled",
        "for rest of benchmarks and vice versa."
    ],
    "monitor.timeseries": [
        "A string that specifies which timeseries metrics must go into a log file. Metrics are separated with comma (,). Each",
        "metric specification consists of three or four fields separated with colon (:) - 'name:type:index_range'. The name",
        "specifies timeseries name. The field in log file will be composed as 'results.use.$name'. Type specifies how values",
        "that come from monitor need to be cast (std, int, float or bool).",
        "Values from resource monitor come as a whitespace separated string. The index range specifies how that maps to a timeseries",
        "name. It can be a single integer(for instance time:str:1) specfying exact index or a index and number of elements that should",
        "be appended to a timeseries item. Number of elements may not be present what means scan until the end of list is reached ",
        "(for instance gpu:float:8:2 or gpu:float:8:). If number of elements is specified, a timeseries will contain items that will",
        "be lists event though number of elements may be 1."
    ],
    "monitor.pid_folder": [
        "A host folder that will be used by a resource monitor and benchmarking scripts to communicate process id that should be monitored.",
        "Users need to specify this parameter of they want to change default path."
    ],
    "monitor.backend_pid_folder": [
        "This is a host or docker folder that will be used by a benchmarking scripts. Will be different from `monitor.pid_folder` if",
        "containerized benchmark is performed. Users must not change this parameter."
    ],
    "monitor.launcher": "A path to an embedded resource monitor.",

    "runtime.limit_resources": "Something that limits process resources. It's used like ${runtime.limit_resources} ${runtime.bind_proc} command with parameters.",
    "runtime.bind_proc": [
        "Can be used to spicify process binding commands like numactl or taskset.",
        "In general, it's any command that should launch the framework. May be a debugger probably.",
        "It's used like ${runtime.limit_resources} ${runtime.bind_proc} command with parameters."
    ],
    "runtime.cuda_cache": "CUDA cache path. May significantly speedup slow startup when a large number of experuments are ran. Set to somewhere in /dev/shm.",

    "sys.plan_builder.var_order": "Order in which plan builder varies variables doing Cartesian product.",
    "sys.plan_builder.method": "Method to build multiple experiments, the only supported value is 'cartesian_product'.",
    "exp.sys_info": [
        "A comma separated string that defines what tools should be used to collect system wide information. A default empty",
        "value means no system information is collected. To collect all information use:",
        "    -Pexp.sys_info='\"inxi,cpuinfo,meminfo,lscpu,nvidiasmi\"'",
        "The following source of information are supported:",
        "    inxi       The inxi must be available (https://github.com/smxi/inxi). It is an output of 'inxi -Fbfrlp'.",
        "    cpuinfo    Content of /proc/cpuinfo",
        "    meminfo    Content of /proc/meminfo",
        "    lscpu      Output of 'lscpu'",
        "    nvidiasmi  Output of '/usr/bin/nvidia-smi -q'",
        "The information is stored in a 'hw' namespace i.e. hw.inxi, hw.cpuinfo, hw.meminfo, hw.lscpu and hw.nvidiasmi.",
        "In addition, a complete output in a json format can be obtained with:",
        "    python ./python/dlbs/experimenter.py sysinfo"
    ]
}